\section{Goals and Research Questions}
\label{goal}
The goal of this study is to assess whether a probabilistic approach as well as different training techniques results in an energy-efficient model with less impact on the environment. Based on this, after an analysis on the topic, we have structured our research around three research questions (\textbf{RQs}).

\medskip

\fcolorbox{black}{blue!5!white}{

\begin{tabular}[t]{p{0.825\columnwidth}}
    \textbf{RQ$_{1}$.} \textit{Could a probabilistic approach lead to more energy efficient Transformer-based model?}
\end{tabular}
}

\medskip
\fcolorbox{black}{blue!5!white}{
\begin{tabular}[t]{p{0.825\columnwidth}}
    \textbf{RQ$_{2}$.} \textit{Does removing parts of the information based on a heuristic allow the model to converge sooner?}
\end{tabular}
}

\medskip
\fcolorbox{black}{blue!5!white}{
\begin{tabular}[t]{p{0.825\columnwidth}}
    \textbf{RQ$_{3}$.} \textit{Can different training techniques lead to lower energy consumption while preserving performance?}
\end{tabular}
}

\medskip

With the first research question (\textbf{RQ$_{1}$}), based on work of Movellan et al. \cite{DBLP:journals/corr/abs-2010-15583}, we want to show that it is possible to use the attention mechanism in a mixture model treating the keys and values as the data points to cluster, and the queries as the test points for which the probability density function is estimated. The attention weights can be used as the mixture weights, and the values associated with each key can be used as the component PDFs. The mixture model can be trained using the MLE method, with the attention mechanism serving as the mixture density estimator.

Through this described methodology, our goal is to test whether a probabilistic approach applied to a Transformer-based model is able to make it energy efficient. The results that will be given to us from these experiments will only be a baseline, and that is why \textbf{RQ$_{2}$} was introduced, so that we can test whether it is possible to optimize the energy efficiency not only at the training level, but also on the size of the input by going to eliminate part of the information using a heuristic. 

In particular, we will try to answer \textbf{RQ$_{2}$} by attempting to implement a feature selection layer that use an heuristic to select which part of the image pass to the Vision Transformer model. In a Vision Transformer architecture, each image is divided into $n$ patches. Given a threshold $k < n$ we are going to select the a $k$ sample of patches according to the probability distribution described by the contrast, variance or entropy value of the patches.

In particular, in order to validate our hypotheses, it is not possible to rely only on one instance of training because this is not enough. In fact, this is why (\textbf{RQ$_{3}$}) was introduced, in which we are going to test if different approaches of training in the Image Recognition task can lead to a faster training and thus lead to lower electricity consumption. In common practice, a transformer-based model first goes through a pre-training phase on a large amount of data and then is fine-tuned for the specific task. We are going to train the model in a different way hoping that it will need less time to achieve the same performance. Firstly a pre-training phase will be done but for much less epochs compared to the common practice, then the model will be trained using the hard-negative contrastive learning where the model is trained to distinguish between a positive pair (consisting of two samples that are similar or belong to the same class) and a hard-negative pair  (consisting of two samples that are dissimilar or belong to different classes) and only then will a specific fine-tuning phase be carried out.
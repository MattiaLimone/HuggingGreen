\section{Future plans}
\label{future}
In this paper, we presented HuggingGreen, a probabilistic attention-based method with the aim of reducing the environmental impact of transformer-based neural network models. However, there are several avenues for future research that could build upon our work and further advance the field of energy-efficient machine learning.

One potential direction for future research is to investigate the effectiveness of HuggingGreen on larger and more complex datasets. Probably it is possible that the performance of our method may degrade when faced with larger datasets or datasets with more different input features. Therefore, future research could focus on developing modifications to HuggingGreen that enable it to scale to larger datasets without sacrificing performance.

Another area for future investigation is the potential for incorporating HuggingGreen into existing deep learning architectures (e.g. ResNet50, EfficientNetV2).

Finally, future research could explore the application of HuggingGreen to real-world energy-efficient machine learning problems because it is important to validate our method on real-world sceniario to ensure its practical effectiveness. In particular, future research could focus on the deploy phase of an HuggingGreen-based model.

In summary, we hope that our work on HuggingGreen provides a foundation for future research on energy-efficient machine learning. We believe that the potential for further improvements and applications of this method is vast, and we look forward to seeing how the field of energy-efficient machine learning continues to evolve in the coming years.
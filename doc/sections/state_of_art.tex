\section{Related works}
\label{relatedwork}
Convolutional neural networks (CNNs) have been widely used for image recognition tasks, but recent studies have shown that vision transformers (ViTs) can outperform CNNs on several benchmark datasets \cite{9790134}. However, ViTs have high computational and memory requirements, making them challenging to deploy on resource-constrained devices.

Convolutional Neural Networks (CNN) are a class of artificial neural networks that were primarily developed for image and video processing. They automatically detect important features in an image using convolution filters, which produce an activation map that represents the relevant image features. CNNs were first introduced by Yann LeCun et al. \cite{lecun1995convolutional} in 1998, and have since become one of the most effective methods for image processing. They have been successfully applied in a wide range of applications, including image recognition, text classification, and medical image analysis.  

Transformers are a type of neural network introduced in 2017 by Vaswani et al.\cite{DBLP:journals/corr/VaswaniSPUJGKP17}that use an attention mechanism to model the relationships between different parts of a sequence. They are particularly effective for natural language processing tasks, and have achieved state-of-the-art performance on tasks such as machine translation and text classification. Transformers are highly parallelizable and have also been successfully applied to other types of sequential data, such as images and music.

To address the issues of energy efficiency, several techniques have been proposed to reduce the energy consumption of ViTs. One approach is to use algorithms or heuristics that delete the large amount of redundancy present in self-attention operations. 

For example Yangfan Li et al.\cite{li2022divit} propose a delta patch encoding which expresses information in a compressed, more space-efficient and communication-efficient manner and a novel algorithm design of differential attention that leverage this patch locality to avoid these redundancies without loss of accuracy. 

Instead Jing Liu et al. \cite{liu2022ecoformer} propose a new binarization paradigm customized to high-dimensional
softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. In this study based on PVTv2-B0 and
ImageNet-1K datasets EcoFormer achieves a 73\% reduction in on-chip energy footprint with only a slight performance drop of 0.33\% compared to the standard attention.

In addition, some studies instead have approached the problem of energy consumption in a different way, for example, Ibrahim et al. \cite{ibrahim2022imagesig} propose ImageSign, a methodology in which images are processed as signatures and processed through one-dimensional convolution (conv1d). Through this study, the authors were able to show that on some datasets they were able to drastically reduce both the number of parameters and the size of the model compared to a ViT, from 4,915,401 to 37,112 parameters and from 59.5 MB to 0.6 MB, respectively. In addition, they also managed to reduce the number of FLOPs from 4.65 to 1.69 without having a loss of accuracy; in fact, in one of their studies, permonances improved compared to ViTs, from 75.23\% to 95.02\%.

In 2021 Gabbur et al. \cite{DBLP:journals/corr/abs-2010-15583} proposed a probabilistic interpretation of attention and suggested the use of Expectation Maximization algorithms for online adaptation of key and value model parameters, which can improve transformer model performance in tasks that require adaptation to new information during inference. Based on this work we have implemented their solution in a ViT architecture to address if the solution can be used in offline learning and  Nguyen et al. \cite{DBLP:journals/corr/abs-2110-08678,} proposed a novel transformer architecture called Transformer-MGK, which replaces redundant attention heads in transformers with a mixture of Gaussian keys. Transformer-MGK accelerates training and inference, has fewer parameters, and achieves comparable or better accuracy across tasks than its conventional transformer counterpart. This work highlights the potential of using mixture models to improve transformer performance and reduce computational complexity.

Overall, these studies demonstrate the importance of addressing the energy efficiency of ViTs, and highlight several techniques that can be used to reduce their computational and memory requirements. Our work builds on these prior studies by proposing a new technique that combines pruning and knowledge distillation to improve the energy efficiency of ViTs while maintaining their accuracy.
\section{Related works}
\label{relatedwork}
Convolutional neural networks (CNNs) have been widely used for image recognition tasks, but recent studies have shown that vision transformers (ViTs) can outperform CNNs on several benchmark datasets \cite{9790134}. However, ViTs have high computational and memory requirements, making them challenging to deploy on resource-constrained devices.



Transformers are a type of deep learning model that has revolutionized the field of natural language processing (NLP) and has also been used in computer vision and other fields.

Transformers were first introduced in 2017 by Vaswani et al.\cite{DBLP:journals/corr/VaswaniSPUJGKP17}. The key innovation of transformers is the use of self-attention mechanisms instead of recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which were the predominant models for NLP at the time.

Self-attention is a mechanism that allows the model to weigh the importance of different parts of the input sequence when making predictions. This mechanism allows the model to handle long-range dependencies much better than RNNs or CNNs, which makes them ideal for NLP tasks such as language modeling, machine translation, and sentiment analysis.

Transformers consist of an encoder and a decoder. The encoder takes in the input sequence and produces a sequence of hidden representations, while the decoder takes in the output of the encoder and generates the output sequence. The key to the success of transformers is the use of multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously.

Vision Transformer (ViT), introduced by Dosovitskiy et al.  \cite{DBLP:journals/corr/abs-2010-11929}, is a deep learning model that uses the transformer architecture for image classification. 

Traditionally, convolutional neural networks (CNNs) have been the dominant model architecture for image classification tasks. However, ViT uses the transformer architecture to process image data without using any convolutional layers.

In ViT, the image is divided into a set of fixed-size patches, and each patch is treated as a token. These tokens are then processed by a transformer encoder, which allows the model to capture global information about the image. The transformer encoder consists of multiple layers, each containing a multi-head self-attention mechanism and a position-wise feedforward network.

ViT achieves state-of-the-art results on several benchmark image classification datasets, including ImageNet and CIFAR-100. One of the benefits of ViT is that it allows for better generalization to out-of-distribution data since it relies on global features instead of local patterns.


To address the issues of energy efficiency, several techniques have been proposed to reduce the energy consumption of ViTs. One approach is to use algorithms or heuristics that delete the large amount of redundancy present in self-attention operations. 

For example Yangfan Li et al.\cite{li2022divit} propose a delta patch encoding which expresses information in a compressed, more space-efficient and communication-efficient manner and a novel algorithm design of differential attention that leverage this patch locality to avoid these redundancies without loss of accuracy. 

Instead Jing Liu et al. \cite{liu2022ecoformer} propose a new binarization paradigm customized to high-dimensional
softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. In this study based on PVTv2-B0 and
ImageNet-1K datasets EcoFormer achieves a 73\% reduction in on-chip energy footprint with only a slight performance drop of 0.33\% compared to the standard attention.

In addition, some studies instead have approached the problem of energy consumption in a different way, for example, Ibrahim et al. \cite{ibrahim2022imagesig} propose ImageSign, a methodology in which images are processed as signatures and processed through one-dimensional convolution (conv1d). Through this study, the authors were able to show that on some datasets they were able to drastically reduce both the number of parameters and the size of the model compared to a ViT, from 4,915,401 to 37,112 parameters and from 59.5 MB to 0.6 MB, respectively. In addition, they also managed to reduce the number of FLOPs from 4.65 to 1.69 without having a loss of accuracy; in fact, in one of their studies, permonances improved compared to ViTs, from 75.23\% to 95.02\%.

In 2021 Gabbur et al. \cite{DBLP:journals/corr/abs-2010-15583} proposed a probabilistic interpretation of attention and suggested the use of Expectation Maximization algorithms for online adaptation of key and value model parameters, which can improve transformer model performance in tasks that require adaptation to new information during inference. Based on this work we have implemented their solution in a ViT architecture to address if the solution can be used in offline learning and  Nguyen et al. \cite{DBLP:journals/corr/abs-2110-08678,} proposed a novel transformer architecture called Transformer-MGK, which replaces redundant attention heads in transformers with a mixture of Gaussian keys. Transformer-MGK accelerates training and inference, has fewer parameters, and achieves comparable or better accuracy across tasks than its conventional transformer counterpart. This work highlights the potential of using mixture models to improve transformer performance and reduce computational complexity.

Overall, these studies demonstrate the importance of addressing the energy efficiency of ViTs, and highlight several techniques that can be used to reduce their computational and memory requirements. Our work builds on these prior studies by proposing a new technique that combines pruning and knowledge distillation to improve the energy efficiency of ViTs while maintaining their accuracy.